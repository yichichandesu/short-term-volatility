{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yichichandesu/short-term-volatility/blob/main/LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWoQXP3i_0c"
      },
      "source": [
        "# 0. Set-Up and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YFmNWcNi3lc",
        "outputId": "b8e465e0-2787-48bd-840f-72d272f80a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading Raw Data from: /content/drive/MyDrive/Colab Notebooks/crypto_data/monarq_data\n",
            "Saving Parquets to: /content/drive/MyDrive/Colab Notebooks/crypto_data/processed_final\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Paths\n",
        "BASE_PATH = Path('/content/drive/MyDrive/Colab Notebooks/crypto_data')\n",
        "MONARQ_PATH = BASE_PATH / 'monarq_data'\n",
        "FUNDING_PATH = BASE_PATH / 'processed_funding' # Where we saved the funding parquets earlier\n",
        "OUTPUT_PATH = BASE_PATH / 'processed_final'    # Where we will save the final features\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 3. Define Symbols & Constants\n",
        "TARGET_SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\"]\n",
        "DATA_TYPES = [\"trade_1min\", \"level1_1min\", \"book_1min\", \"openinterest_1min\"]\n",
        "TYPE_PREFIX = {\n",
        "    \"trade_1min\": \"tr\",\n",
        "    \"level1_1min\": \"l1\",\n",
        "    \"book_1min\": \"l2\",\n",
        "    \"openinterest_1min\": \"oi\"\n",
        "}\n",
        "\n",
        "# 4. Processed Parquet Files Check\n",
        "processed_funding_check = True\n",
        "processed_final_check = True\n",
        "\n",
        "print(f\"Reading Raw Data from: {MONARQ_PATH}\")\n",
        "print(f\"Saving Parquets to: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SWADEuRumxnq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VolatilityLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of features (e.g., 10 tech indicators).\n",
        "            hidden_size (int): Number of neurons in the LSTM hidden state.\n",
        "            num_layers (int): Number of stacked LSTM layers.\n",
        "            output_size (int): Number of values to predict (usually 1 for volatility).\n",
        "            dropout (float): Dropout rate to prevent overfitting.\n",
        "        \"\"\"\n",
        "        super(VolatilityLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 1. The LSTM Layer\n",
        "        # batch_first=True means input shape is (Batch, Seq_Len, Features)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 2. The Fully Connected (Linear) Layer\n",
        "        # Maps the hidden state to the final volatility output\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states with zeros (optional, PyTorch does this by default)\n",
        "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        # out shape: (Batch_Size, Seq_Length, Hidden_Size)\n",
        "        out, _ = self.lstm(x)\n",
        "\n",
        "        # Decode the hidden state of the LAST time step\n",
        "        # We only care about the final prediction after seeing the whole window\n",
        "        last_time_step_out = out[:, -1, :]\n",
        "\n",
        "        # Pass through the Linear layer to get the final prediction\n",
        "        prediction = self.fc(last_time_step_out)\n",
        "\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVT1HLa5jEAT"
      },
      "source": [
        "# 1. Microstructure Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4FnjRRsXi4Wb"
      },
      "outputs": [],
      "source": [
        "def process_microstructure_and_save(symbol):\n",
        "    print(f\"\\n=== Processing Microstructure: {symbol} ===\")\n",
        "\n",
        "    # 1. Load and Merge Raw Data Types (Trade, L1, L2, OI)\n",
        "    # (This part is the same as before - loading CSVs)\n",
        "    dfs = []\n",
        "    for dt in DATA_TYPES:\n",
        "        folder = MONARQ_PATH / dt / symbol\n",
        "        prefix = TYPE_PREFIX.get(dt, dt)\n",
        "        files = sorted(folder.glob(\"*.csv.gz\"))\n",
        "\n",
        "        if not files: continue\n",
        "\n",
        "        type_dfs = []\n",
        "        for f in files:\n",
        "            try:\n",
        "                temp = pd.read_csv(f)\n",
        "                type_dfs.append(temp)\n",
        "            except: pass\n",
        "\n",
        "        if type_dfs:\n",
        "            df_type = pd.concat(type_dfs, ignore_index=True)\n",
        "            if \"ts_end\" in df_type.columns:\n",
        "                df_type[\"ts_end\"] = pd.to_datetime(df_type[\"ts_end\"], unit=\"ms\", utc=True)\n",
        "                df_type = df_type.sort_values(\"ts_end\").set_index(\"ts_end\")\n",
        "                df_type = df_type[~df_type.index.duplicated(keep=\"last\")]\n",
        "                df_type = df_type.rename(columns={c: f\"{prefix}__{c}\" for c in df_type.columns})\n",
        "                dfs.append(df_type)\n",
        "\n",
        "    if not dfs: return False\n",
        "\n",
        "    print(\"Joining data types...\")\n",
        "    df = dfs[0]\n",
        "    for d in dfs[1:]:\n",
        "        df = df.join(d, how=\"outer\")\n",
        "\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 2. FULL FEATURE ENGINEERING (Ported from Person A's 'rebuild_all_pdf_features')\n",
        "    # ==============================================================================\n",
        "    print(\"   wv Calculating ALL Features...\")\n",
        "    eps = 1e-8\n",
        "\n",
        "    # --- A. Basic Price/Vol ---\n",
        "    df[\"dt\"] = df.index\n",
        "    df[\"symbol\"] = symbol\n",
        "    if \"tr__close_price\" in df.columns:\n",
        "        df[\"ret_1m\"] = df[\"tr__close_price\"].pct_change()\n",
        "        df[\"abs_ret_1m\"] = df[\"ret_1m\"].abs() # Added from Cell 21\n",
        "        df[\"rv_30m\"] = df[\"ret_1m\"].pow(2).rolling(30, min_periods=10).sum()\n",
        "\n",
        "    if \"tr__volume\" in df.columns:\n",
        "        df[\"log_volume\"] = np.log1p(df[\"tr__volume\"].astype(float)) # Added from Cell 21\n",
        "\n",
        "    # --- B. Order Flow Imbalance (OFI) ---\n",
        "    if \"tr__buy_volume\" in df.columns and \"tr__sell_volume\" in df.columns:\n",
        "        df[\"OFI_raw\"] = df[\"tr__buy_volume\"] - df[\"tr__sell_volume\"]\n",
        "\n",
        "        # OFI Derived Features\n",
        "        df[\"OFI_std_30m\"] = df[\"OFI_raw\"].rolling(30, min_periods=10).std()\n",
        "\n",
        "        ofi_lag10 = df[\"OFI_raw\"].shift(10)\n",
        "        df[\"OFI_roc_10m\"] = (df[\"OFI_raw\"] - ofi_lag10) / (ofi_lag10.abs() + eps)\n",
        "\n",
        "        # VPIN Proxy (Volume-Synchronized Probability of Informed Trading)\n",
        "        buy_v = df[\"tr__buy_volume\"]\n",
        "        sell_v = df[\"tr__sell_volume\"]\n",
        "        df[\"VPIN_proxy\"] = (buy_v - sell_v).abs() / (buy_v + sell_v + eps)\n",
        "\n",
        "    # --- C. L1 Order Book Features ---\n",
        "    if \"l1__close_bid_size\" in df.columns:\n",
        "        bid_sz = df[\"l1__close_bid_size\"]\n",
        "        ask_sz = df[\"l1__close_ask_size\"]\n",
        "        df[\"L1_Imbalance\"] = (bid_sz - ask_sz) / (bid_sz + ask_sz + eps)\n",
        "\n",
        "        # Liquidity Weighted OFI\n",
        "        if \"OFI_raw\" in df.columns:\n",
        "            depth = df.get(\"l1__mean_bid_size\", 0) + df.get(\"l1__mean_ask_size\", 0)\n",
        "            df[\"Liquidity_Weighted_OFI\"] = df[\"OFI_raw\"] * depth\n",
        "\n",
        "    if \"l1__mean_spread\" in df.columns:\n",
        "        df[\"Mean_Spread\"] = df[\"l1__mean_spread\"]\n",
        "        df[\"Spread_ma_15m\"] = df[\"l1__mean_spread\"].rolling(15, min_periods=5).mean()\n",
        "\n",
        "    # --- D. Advanced Microstructure ---\n",
        "    # VWAP Deviation\n",
        "    if \"tr__vwap\" in df.columns and \"tr__close_price\" in df.columns:\n",
        "        df[\"VWAP_deviation\"] = (df[\"tr__vwap\"] - df[\"tr__close_price\"]) / (df[\"tr__close_price\"] + eps)\n",
        "\n",
        "    # Trade Intensity\n",
        "    if \"tr__trade_count\" in df.columns:\n",
        "        df[\"Trade_Intensity\"] = df[\"tr__trade_count\"]\n",
        "\n",
        "    # Price Impact\n",
        "    if \"tr__close_price\" in df.columns and \"tr__volume\" in df.columns:\n",
        "        ret_15m = df[\"tr__close_price\"].pct_change(15)\n",
        "        vol_15m = df[\"tr__volume\"].rolling(15, min_periods=5).sum()\n",
        "        df[\"Price_Impact_per_Volume\"] = ret_15m.abs() / (vol_15m + eps)\n",
        "\n",
        "    # --- E. L2 Order Book (Depth Ratios) ---\n",
        "    # Only if Level 2 data exists\n",
        "    if \"l2__bid_10000K_fill_dsize\" in df.columns:\n",
        "        b_10m = df[\"l2__bid_10000K_fill_dsize\"]\n",
        "        a_10m = df[\"l2__ask_10000K_fill_dsize\"]\n",
        "        df[\"depth_ratio_10M\"] = b_10m / (a_10m + eps)\n",
        "        df[\"l2_imbalance_10M\"] = (b_10m - a_10m) / (b_10m + a_10m + eps)\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 3. SAVE TO PARQUET\n",
        "    # ==============================================================================\n",
        "    out_file = OUTPUT_PATH / f\"{symbol}_micro_features.parquet\"\n",
        "    df.to_parquet(out_file)\n",
        "    print(f\"Saved {len(df)} rows to {out_file.name} (With ALL features)\")\n",
        "\n",
        "    # 4. Clean Memory\n",
        "    del df\n",
        "    del dfs\n",
        "    gc.collect()\n",
        "    return True\n",
        "\n",
        "if not processed_funding_check:\n",
        "  # --- RUN THE UPDATE ---\n",
        "  for sym in TARGET_SYMBOLS:\n",
        "      process_microstructure_and_save(sym)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoTodqVCjRxM"
      },
      "source": [
        "# 3. Merging Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oJkrSGlsjOnS"
      },
      "outputs": [],
      "source": [
        "def finalize_dataset(symbol):\n",
        "    print(f\"\\n=== Finalizing {symbol} ===\")\n",
        "\n",
        "    # 1. Load Microstructure Features (Fast Read)\n",
        "    micro_file = OUTPUT_PATH / f\"{symbol}_micro_features.parquet\"\n",
        "    if not micro_file.exists():\n",
        "        print(f\"Missing microstructure file for {symbol}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_parquet(micro_file)\n",
        "\n",
        "    # 2. Load and Merge Funding Rate\n",
        "    # Note: Using the processed_funding folder we created previously\n",
        "    fund_file = FUNDING_PATH / f\"{symbol}_funding_clean.parquet\"\n",
        "\n",
        "    if fund_file.exists():\n",
        "        funding_df = pd.read_parquet(fund_file)\n",
        "\n",
        "        # Resample Funding to 1-min (Forward Fill)\n",
        "        funding_df = funding_df.set_index(\"dt\").sort_index()\n",
        "        funding_1min = funding_df[\"rate\"].resample(\"1min\").ffill().to_frame(\"funding_rate\")\n",
        "\n",
        "        # Merge onto main dataframe\n",
        "        # Ensure df has datetime index or column 'dt'\n",
        "        if \"dt\" in df.columns:\n",
        "            df = df.set_index(\"dt\")\n",
        "\n",
        "        df = df.join(funding_1min, how=\"left\")\n",
        "        print(\"gw Funding rate merged.\")\n",
        "    else:\n",
        "        print(\"Funding file not found, filling NaNs.\")\n",
        "        df[\"funding_rate\"] = np.nan\n",
        "\n",
        "    # 3. Create Targets (Future Volatility)\n",
        "    # Target: Realized Volatility 5 mins into the future\n",
        "    if \"ret_1m\" in df.columns:\n",
        "        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=5)\n",
        "        df[\"rv_fwd_5m\"] = df[\"ret_1m\"].pow(2).rolling(window=indexer).sum().pow(0.5)\n",
        "        print(\"Target (rv_fwd_5m) created.\")\n",
        "\n",
        "        # Create Lags (e.g., Volatility 5 mins ago)\n",
        "        df[\"rv_30m_lag5\"] = df[\"rv_30m\"].shift(5)\n",
        "\n",
        "    # 4. Save FINAL Dataset\n",
        "    final_file = OUTPUT_PATH / f\"{symbol}_final_train.parquet\"\n",
        "    df.to_parquet(final_file)\n",
        "    print(f\"Saved Final: {final_file.name}\")\n",
        "\n",
        "    # 5. Purge\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "if not processed_final_check:\n",
        "  # --- RUN PHASE 2 ---\n",
        "  for sym in TARGET_SYMBOLS:\n",
        "      finalize_dataset(sym)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcQR7K29k0IY"
      },
      "source": [
        "# 4. LSTM Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pBqEavLxmoGE"
      },
      "outputs": [],
      "source": [
        "def load_training_data(symbols):\n",
        "    \"\"\"\n",
        "    Efficiently loads and concatenates the finalized parquet files\n",
        "    generated in Phase 2.\n",
        "    \"\"\"\n",
        "    dfs = []\n",
        "    for sym in symbols:\n",
        "        # Construct the path to the final parquet file\n",
        "        file_path = OUTPUT_PATH / f\"{sym}_final_train.parquet\"\n",
        "\n",
        "        if file_path.exists():\n",
        "            print(f\"   Reading {sym}...\")\n",
        "            df = pd.read_parquet(file_path)\n",
        "\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"   Warning: File not found for {sym} at {file_path}\")\n",
        "\n",
        "    if not dfs:\n",
        "        return None\n",
        "\n",
        "    # Concatenate all symbols into one giant dataframe\n",
        "    print(\"   Concatenating symbols...\")\n",
        "    return pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TP5AWZxQqb7G"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "def select_best_features(symbol=\"BTCUSDT\", corr_threshold=0.95, vif_threshold=10, top_n=20):\n",
        "    print(f\"Running 3-Step Feature Selector on {symbol}...\")\n",
        "\n",
        "    # 1. Load Sample and Setup\n",
        "    file_path = OUTPUT_PATH / f\"{symbol}_final_train.parquet\"\n",
        "    if not file_path.exists(): return []\n",
        "\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    # Sample size: 30k rows is a good balance\n",
        "    df_sample = df.iloc[:30000].fillna(0)\n",
        "\n",
        "    target_col = \"rv_fwd_5m\"\n",
        "    drop_cols = [\"symbol\", \"dt\", \"ts_end\", \"source_file\", target_col]\n",
        "\n",
        "    # Filter numeric only\n",
        "    feature_cols = [c for c in df_sample.columns if c not in drop_cols]\n",
        "    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df_sample[c])]\n",
        "\n",
        "    X = df_sample[feature_cols].copy()\n",
        "    y = df_sample[target_col]\n",
        "\n",
        "    # Cleaning for numerical stability\n",
        "    X.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "    X = X.loc[:, (X != X.iloc[0]).any()] # Zero variance check\n",
        "    X.dropna(inplace=True)\n",
        "\n",
        "    print(f\"   Starting with {X.shape[1]} clean features.\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 1: CORRELATION FILTER (Coarse Cleaning)\n",
        "    # ==============================================================================\n",
        "    print(f\"   1/3: Correlation Filter (Corr > {corr_threshold})\")\n",
        "    corr_matrix = X.corr().abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    corr_to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
        "    X_corr_clean = X.drop(columns=corr_to_drop)\n",
        "\n",
        "    print(f\"      Dropped {len(corr_to_drop)} features. Remaining: {X_corr_clean.shape[1]}\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 2: VIF FILTER (Fine Cleaning / Multicollinearity Check)\n",
        "    # ==============================================================================\n",
        "    print(f\"   2/3: VIF Filter (VIF < {vif_threshold})\")\n",
        "\n",
        "    # Add constant for VIF calculation\n",
        "    X_vif = add_constant(X_corr_clean)\n",
        "\n",
        "    dropped = True\n",
        "    while dropped:\n",
        "        dropped = False\n",
        "        vif_data = pd.Series(\n",
        "            [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])],\n",
        "            index=X_vif.columns\n",
        "        ).drop('const', errors='ignore')\n",
        "\n",
        "        max_vif = vif_data.max()\n",
        "        max_feature = vif_data.idxmax()\n",
        "\n",
        "        # If max VIF is too high, drop that feature and re-calculate\n",
        "        if max_vif > vif_threshold and len(X_vif.columns) > 1:\n",
        "            # Only print every few drops to keep output clean\n",
        "            if len(X_vif.columns) % 5 == 0:\n",
        "                print(f\"      ... Dropping '{max_feature}' (VIF={max_vif:.1f})\")\n",
        "\n",
        "            X_vif = X_vif.drop(columns=[max_feature])\n",
        "            dropped = True\n",
        "\n",
        "    clean_features = [c for c in X_vif.columns if c != 'const']\n",
        "    print(f\"      Remaining unique features: {len(clean_features)}\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 3: RANDOM FOREST RANKING (Final Relevance Check)\n",
        "    # ==============================================================================\n",
        "    print(\"   3/3: Random Forest Ranking\")\n",
        "\n",
        "    # Use the clean feature set\n",
        "    X_final = X[clean_features]\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
        "    rf.fit(X_final, y)\n",
        "\n",
        "    importances = pd.DataFrame({\n",
        "        'Feature': X_final.columns,\n",
        "        'Importance': rf.feature_importances_\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Pick Top N\n",
        "    final_features = importances.head(top_n)['Feature'].tolist()\n",
        "\n",
        "    print(f\"\\nFinal Feature Set ({len(final_features)} features):\")\n",
        "    print(importances.head(10))\n",
        "\n",
        "    # Cleanup\n",
        "    del df, df_sample, X, X_corr_clean, X_vif, X_final, rf\n",
        "    gc.collect()\n",
        "\n",
        "    return final_features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_features_check = True\n",
        "if not best_features_check:\n",
        "  best_features = select_best_features(\"BTCUSDT\")\n",
        "else:\n",
        "  # Best Features via Random Forest only\n",
        "  # best_features = ['abs_ret_1m', 'rv_30m', 'l1__ask_down_ret', 'tr__taker_trade_count', 'tr__sell_trade_count',\n",
        "  #                  'l1__bid_up_ret', 'l1__bid_down_ret', 'l1__tick_count', 'rv_30m_lag5', 'OFI_std_30m', 'Spread_ma_15m',\n",
        "  #                  'l1__ask_up_ret', 'tr__bin_id', 'Trade_Intensity', 'tr__trade_count', 'tr__carryover', 'l1__bin_id',\n",
        "  #                  'l2__bid_1bps_fill_size', 'l2__ask_30bps_fill_size', 'l2__bin_id']\n",
        "\n",
        "  # Best Features via\n",
        "  best_features = ['l1__ask_up_ret', 'abs_ret_1m', 'l1__tick_count', 'Spread_ma_15m', 'rv_30m_lag5',\n",
        " 'OFI_std_30m', 'tr__bin_id', 'l1__l3_updates',  'l2__ask_30bps_fill_size', 'l2__ask_100bps_fill_size',\n",
        " 'tr__ramp_up', 'L1_Imbalance', 'l2__bid_size', 'VWAP_deviation', 'l2__ask_10bps_fill_size',  'ret_1m',\n",
        " 'l2__bid_1bps_fill_size', 'l2__bid_100bps_fill_size', 'VPIN_proxy', 'log_volume']"
      ],
      "metadata": {
        "id": "2oz0TaoqqHlE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rywgsl7rqk7k",
        "outputId": "94d025dc-be0d-49f5-f13b-b4dcd6e28eb3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l1__ask_up_ret',\n",
              " 'abs_ret_1m',\n",
              " 'l1__tick_count',\n",
              " 'Spread_ma_15m',\n",
              " 'rv_30m_lag5',\n",
              " 'OFI_std_30m',\n",
              " 'tr__bin_id',\n",
              " 'l1__l3_updates',\n",
              " 'l2__ask_30bps_fill_size',\n",
              " 'l2__ask_100bps_fill_size',\n",
              " 'tr__ramp_up',\n",
              " 'L1_Imbalance',\n",
              " 'l2__bid_size',\n",
              " 'VWAP_deviation',\n",
              " 'l2__ask_10bps_fill_size',\n",
              " 'ret_1m',\n",
              " 'l2__bid_1bps_fill_size',\n",
              " 'l2__bid_100bps_fill_size',\n",
              " 'VPIN_proxy',\n",
              " 'log_volume']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FurdpFGanJdf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LazyVolatilityDataset(Dataset):\n",
        "    def __init__(self, feature_data, target_data, seq_len=60):\n",
        "        # Store efficient 2D matrices\n",
        "        # We ensure they are float32 to save memory\n",
        "        self.features = torch.tensor(feature_data, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(target_data, dtype=torch.float32)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        # The number of possible windows\n",
        "        return len(self.features) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Create the 3D window on the fly\n",
        "        # Slices from [i] to [i + seq_len]\n",
        "        x_window = self.features[idx : idx + self.seq_len]\n",
        "\n",
        "        # The target is the value at the end of the window sequence\n",
        "        # (or slightly after, depending on how y was shifted)\n",
        "        y_label = self.targets[idx + self.seq_len]\n",
        "\n",
        "        return x_window, y_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lq3JK-HEkxhZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def prepare_lstm_data(symbols, selected_features, batch_size=2048, seq_len=60):\n",
        "    print(\"Loading Optimized Data...\")\n",
        "\n",
        "    # 1. LOAD DATA\n",
        "    full_df = load_training_data(symbols)\n",
        "    if full_df is None: return None, None, None, None\n",
        "\n",
        "    # 2. SORT\n",
        "    if \"dt\" in full_df.columns:\n",
        "        full_df = full_df.sort_values([\"dt\", \"symbol\"])\n",
        "\n",
        "    target_col = \"rv_fwd_5m\"\n",
        "\n",
        "    # 3. FILTER COLUMNS\n",
        "    keep_cols = selected_features + [target_col]\n",
        "    keep_cols = [c for c in keep_cols if c in full_df.columns]\n",
        "\n",
        "    print(f\"   Filtering: Keeping {len(keep_cols)} columns...\")\n",
        "    data_df = full_df[keep_cols].astype(np.float32)\n",
        "\n",
        "    # =======================================================\n",
        "    # NEW: CLEANING BLOCK (Fixes NaN Loss)\n",
        "    # =======================================================\n",
        "    print(\"   Cleaning NaNs and Infinity...\")\n",
        "\n",
        "    # 1. Replace Infinity with NaN (Crucial for financial data)\n",
        "    data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # 2. Drop rows with ANY NaNs (Safest approach)\n",
        "    # This removes the rows at the end of the dataset where future targets are missing\n",
        "    original_len = len(data_df)\n",
        "    data_df.dropna(inplace=True)\n",
        "\n",
        "    print(f\"   Dropped {original_len - len(data_df)} bad rows.\")\n",
        "    # =======================================================\n",
        "\n",
        "    # Extract Target\n",
        "    y_all = data_df[target_col].values\n",
        "    X_df = data_df.drop(columns=[target_col])\n",
        "\n",
        "    # Clean Memory\n",
        "    del full_df\n",
        "    gc.collect()\n",
        "\n",
        "    # 4. SPLIT INDICES\n",
        "    n = len(X_df)\n",
        "    train_end = int(n * 0.70)\n",
        "    val_end = int(n * 0.85)\n",
        "\n",
        "    # 5. SCALE\n",
        "    print(\"   Scaling...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_df.iloc[:train_end])\n",
        "    X_val = scaler.transform(X_df.iloc[train_end:val_end])\n",
        "    X_test = scaler.transform(X_df.iloc[val_end:])\n",
        "\n",
        "    y_train = y_all[:train_end]\n",
        "    y_val = y_all[train_end:val_end]\n",
        "    y_test = y_all[val_end:]\n",
        "\n",
        "    del X_df, y_all\n",
        "    gc.collect()\n",
        "\n",
        "    # 6. CREATE LOADERS\n",
        "    print(\"   wrapping in DataLoaders...\")\n",
        "    train_loader = DataLoader(LazyVolatilityDataset(X_train, y_train, seq_len), batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(LazyVolatilityDataset(X_val, y_val, seq_len), batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = DataLoader(LazyVolatilityDataset(X_test, y_test, seq_len), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, len(selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bxwidz1BmLim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121d9196-f41c-432b-f94a-5a63c3d2b458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Optimized Data...\n",
            "   Reading BTCUSDT...\n",
            "   Reading ETHUSDT...\n",
            "   Concatenating symbols...\n",
            "   Filtering: Keeping 21 columns...\n",
            "   Cleaning NaNs and Infinity...\n",
            "   Dropped 4464 bad rows.\n",
            "   Scaling...\n",
            "   wrapping in DataLoaders...\n",
            "Data ready. Input Dimension: 20\n"
          ]
        }
      ],
      "source": [
        "train_dl, val_dl, test_dl, input_dim = prepare_lstm_data(TARGET_SYMBOLS, best_features)\n",
        "\n",
        "print(f\"Data ready. Input Dimension: {input_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6QXbCP7qOdq"
      },
      "source": [
        "# 5. LSTM Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xIn6UPz8qUmy"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "def train_lstm_model(model, train_loader, val_loader, epochs=20, lr=0.001, device=None, save_path=\"best_lstm.pth\"):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Training on: {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # --- FIX 1: Initialize history lists ---\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y.view(-1, 1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss) # Store history\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for val_x, val_y in val_loader:\n",
        "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
        "                outputs = model(val_x)\n",
        "                loss = criterion(outputs, val_y.view(-1, 1))\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss) # Store history\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | Time: {epoch_time:.0f}s\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"   New best model saved.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "    print(\"Training Complete.\")\n",
        "\n",
        "    # --- FIX 2: Return the lists ---\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yzk7Qe6omP0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cf5e20-dc18-46f2-e31c-ba3401a0b0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Input Features: 20\n",
            "Starting Training Loop...\n",
            "Training on: cuda\n",
            "Epoch [1/15] | Train Loss: 0.000017 | Val Loss: 0.000001 | Time: 39s\n",
            "   New best model saved.\n",
            "Epoch [2/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 40s\n",
            "Epoch [3/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 40s\n",
            "Epoch [4/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "   New best model saved.\n",
            "Epoch [5/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "Epoch [6/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 37s\n",
            "Epoch [7/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "Epoch [8/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "   New best model saved.\n",
            "Epoch [9/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 37s\n",
            "Epoch [10/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "Epoch [11/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "Epoch [12/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "Epoch [13/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 38s\n",
            "Epoch [14/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 39s\n",
            "Epoch [15/15] | Train Loss: 0.000008 | Val Loss: 0.000001 | Time: 37s\n",
            "   New best model saved.\n",
            "Training Complete.\n",
            "Done! Best model saved as 'best_lstm_model.pth'\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 2048\n",
        "SEQ_LEN = 60\n",
        "\n",
        "if train_dl is not None:\n",
        "    # Dynamically get Input Dimension\n",
        "    sample_x, _ = next(iter(train_dl))\n",
        "    input_dim = sample_x.shape[2]\n",
        "    print(f\"Detected Input Features: {input_dim}\")\n",
        "\n",
        "    # Initialize the Model\n",
        "    model = VolatilityLSTM(\n",
        "        input_size=input_dim,\n",
        "        hidden_size=64,\n",
        "        num_layers=2,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    # Start Training\n",
        "    print(\"Starting Training Loop...\")\n",
        "    train_loss, val_loss = train_lstm_model(\n",
        "        model,\n",
        "        train_dl,\n",
        "        val_dl,\n",
        "        epochs=15,\n",
        "        lr=0.001,\n",
        "        save_path=\"best_lstm_model.pth\"\n",
        "    )\n",
        "\n",
        "    print(\"Done! Best model saved as 'best_lstm_model.pth'\")\n",
        "else:\n",
        "    print(\"Failed to load data. Please check your parquet files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating"
      ],
      "metadata": {
        "id": "5ztrG8X_xFjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "def evaluate_and_plot(model, test_loader, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    print(\"Running Inference on Test Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "\n",
        "            # Get model prediction\n",
        "            preds = model(batch_x)\n",
        "\n",
        "            # Store results (move to CPU and convert to list)\n",
        "            predictions.extend(preds.cpu().numpy().flatten())\n",
        "            actuals.extend(batch_y.numpy().flatten())\n",
        "\n",
        "    # --- METRICS ---\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = math.sqrt(mse)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "\n",
        "    print(f\"\\nFINAL RESULTS:\")\n",
        "    print(f\"   RMSE: {rmse:.6f}\")\n",
        "    print(f\"   MAE:  {mae:.6f}\")\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    # Plot only the first 200 points so we can see the details clearly\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(actuals[:200], label=\"Actual Volatility\", color='black', alpha=0.7)\n",
        "    plt.plot(predictions[:200], label=\"LSTM Prediction\", color='red', linestyle='--')\n",
        "    plt.title(\"LSTM Forecast vs Actual (First 200 Test Points)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "# --- RUN IT ---\n",
        "# 1. Load the best saved model\n",
        "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
        "\n",
        "# 2. Evaluate\n",
        "preds, acts = evaluate_and_plot(model, test_dl)"
      ],
      "metadata": {
        "id": "mwyJ8zXNr64f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jaRcNjS0BJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Define the \"Search Space\" (The menu of options)\n",
        "param_grid = {\n",
        "    \"hidden_size\": [32, 64, 128],       # How big is the brain?\n",
        "    \"num_layers\": [1, 2],               # How deep is the brain?\n",
        "    \"dropout\": [0.0, 0.2, 0.5],         # How much to prevent memorization?\n",
        "    \"learning_rate\": [0.001, 0.0005, 0.0001], # How fast to learn?\n",
        "    \"batch_size\": [512, 1024, 2048]     # How big are the chunks?\n",
        "}\n",
        "\n",
        "def run_hyperparameter_tuning(num_trials=5, epochs_per_trial=3):\n",
        "    print(f\"Starting Random Search for {num_trials} trials...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "        # A. Pick Random Hyperparameters\n",
        "        params = {k: random.choice(v) for k, v in param_grid.items()}\n",
        "        print(f\"\\n--- Trial {trial+1}/{num_trials} ---\")\n",
        "        print(f\"   Settings: {params}\")\n",
        "\n",
        "        # B. Prepare Data (Re-load if batch size changes)\n",
        "        # Note: We use your existing function\n",
        "        train_loader, val_loader, test_loader, input_dim = prepare_lstm_data(\n",
        "            TARGET_SYMBOLS,\n",
        "            best_features, # Use the features we selected earlier\n",
        "            batch_size=params[\"batch_size\"],\n",
        "            seq_len=60\n",
        "        )\n",
        "\n",
        "        # C. Initialize Model\n",
        "        model = VolatilityLSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=params[\"hidden_size\"],\n",
        "            num_layers=params[\"num_layers\"],\n",
        "            dropout=params[\"dropout\"]\n",
        "        )\n",
        "\n",
        "        # D. Train (Short run to check potential)\n",
        "        train_losses, val_losses = train_lstm_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            epochs=epochs_per_trial,\n",
        "            lr=params[\"learning_rate\"],\n",
        "            save_path=f\"trial_{trial}_model.pth\" # Save each trial separately\n",
        "        )\n",
        "\n",
        "        # E. Record Result (Use the final validation loss)\n",
        "        final_val_loss = val_losses[-1]\n",
        "        print(f\"   Result: Val Loss = {final_val_loss:.6f}\")\n",
        "\n",
        "        results.append({\n",
        "            **params,\n",
        "            \"val_loss\": final_val_loss,\n",
        "            \"trial_id\": trial\n",
        "        })\n",
        "\n",
        "    # F. Analyze Results\n",
        "    results_df = pd.DataFrame(results).sort_values(by=\"val_loss\")\n",
        "    print(\"\\nTUNING COMPLETE! Best Configurations:\")\n",
        "    print(results_df.head())\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# --- RUN THE TUNING ---\n",
        "# We run 5 trials for 3 epochs each to save time.\n",
        "# Once you find the best settings, you train THAT one for 15+ epochs.\n",
        "tuning_results = run_hyperparameter_tuning(num_trials=5, epochs_per_trial=3)"
      ],
      "metadata": {
        "id": "s964CN95xH1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GARCH"
      ],
      "metadata": {
        "id": "UJObVRth80Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install arch"
      ],
      "metadata": {
        "id": "-xY8OUb28--f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from arch import arch_model\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "\n",
        "class AutoGARCH:\n",
        "    def __init__(self, p_max=2, q_max=2): # Reduced max to 2 for stability\n",
        "        self.p_max = p_max\n",
        "        self.q_max = q_max\n",
        "        self.best_model = None\n",
        "        self.best_result = None\n",
        "        self.best_params = {}\n",
        "\n",
        "    def fit(self, returns):\n",
        "        best_bic = float('inf')\n",
        "\n",
        "        # Grid Search\n",
        "        for p in range(1, self.p_max + 1):\n",
        "            for q in range(1, self.q_max + 1):\n",
        "                try:\n",
        "                    # CHANGE: rescale=True lets the library fix the \"Code 4\" errors\n",
        "                    model = arch_model(returns, vol='Garch', p=p, q=q, dist='Normal', rescale=True)\n",
        "\n",
        "                    # Silence warnings during the search\n",
        "                    with warnings.catch_warnings():\n",
        "                        warnings.simplefilter(\"ignore\")\n",
        "                        result = model.fit(disp='off', show_warning=False)\n",
        "\n",
        "                    # Only accept valid, converged results\n",
        "                    if result.convergence_flag == 0 and result.bic < best_bic:\n",
        "                        best_bic = result.bic\n",
        "                        self.best_model = model\n",
        "                        self.best_result = result\n",
        "                        self.best_params = {'p': p, 'q': q}\n",
        "                        # print(f\"   Candidate: GARCH({p},{q}) BIC={result.bic:.2f}\")\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if self.best_params:\n",
        "            print(f\"   Best Model: GARCH({self.best_params['p']}, {self.best_params['q']})\")\n",
        "        else:\n",
        "            print(\"   All GARCH models failed to converge. Defaulting to (1,1).\")\n",
        "            # Fallback\n",
        "            self.best_model = arch_model(returns, vol='Garch', p=1, q=1, rescale=True)\n",
        "            self.best_result = self.best_model.fit(disp='off')\n",
        "            self.best_params = {'p': 1, 'q': 1}\n",
        "\n",
        "        return self.best_result\n",
        "\n",
        "def run_optimized_garch(symbols, train_ratio=0.70):\n",
        "    print(\"Starting Optimized GARCH Benchmark...\")\n",
        "\n",
        "    full_df = load_training_data(symbols)\n",
        "    if full_df is None: return\n",
        "\n",
        "    if \"dt\" in full_df.columns:\n",
        "        full_df = full_df.sort_values([\"dt\", \"symbol\"])\n",
        "\n",
        "    # Clean data\n",
        "    data = full_df[[\"ret_1m\", \"rv_fwd_5m\"]].dropna()\n",
        "\n",
        "    # CHANGE: Do NOT multiply by 100 here.\n",
        "    # Pass raw returns and let AutoGARCH handle the scaling.\n",
        "    returns = data[\"ret_1m\"]\n",
        "    actual_vol = data[\"rv_fwd_5m\"]\n",
        "\n",
        "    n = len(returns)\n",
        "    split_point = int(n * train_ratio)\n",
        "    train_returns = returns.iloc[:split_point]\n",
        "\n",
        "    # Run Optimizer\n",
        "    print(f\"\\n   Tuning GARCH on first {len(train_returns)} rows...\")\n",
        "    optimizer = AutoGARCH(p_max=2, q_max=2)\n",
        "    optimizer.fit(train_returns)\n",
        "\n",
        "    best_p = optimizer.best_params.get('p', 1)\n",
        "    best_q = optimizer.best_params.get('q', 1)\n",
        "\n",
        "    # Run Final Model\n",
        "    print(f\"\\n   Testing GARCH({best_p},{best_q}) on Out-of-Sample data...\")\n",
        "    final_model = arch_model(returns, vol='Garch', p=best_p, q=best_q, dist='Normal', rescale=True)\n",
        "    res = final_model.fit(last_obs=split_point, disp='off')\n",
        "\n",
        "    # Forecast\n",
        "    forecasts = res.forecast(horizon=5, start=split_point)\n",
        "\n",
        "    # CRITICAL: Since 'rescale=True', the variance is scaled up.\n",
        "    # We must check the scale factor to normalize it back.\n",
        "    scale = res.scale\n",
        "\n",
        "    # Aggregation logic\n",
        "    variance_forecasts = forecasts.variance.iloc[split_point:]\n",
        "    total_variance = variance_forecasts.sum(axis=1)\n",
        "\n",
        "    # Convert back to original scale\n",
        "    # If scale was 100, variance is 100^2 = 10,000x larger.\n",
        "    # So we take sqrt(var) / scale\n",
        "    garch_pred_vol = np.sqrt(total_variance) / scale\n",
        "\n",
        "    # Evaluate\n",
        "    min_len = min(len(garch_pred_vol), len(actual_vol.iloc[split_point:]))\n",
        "    preds = garch_pred_vol.values[:min_len]\n",
        "    acts = actual_vol.iloc[split_point:].values[:min_len]\n",
        "\n",
        "    mse = mean_squared_error(acts, preds)\n",
        "    rmse = math.sqrt(mse)\n",
        "    mae = mean_absolute_error(acts, preds)\n",
        "\n",
        "    print(f\"\\nOPTIMIZED GARCH RESULTS:\")\n",
        "    print(f\"   RMSE: {rmse:.6f}\")\n",
        "    print(f\"   MAE:  {mae:.6f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(acts[:200], label=\"Actual Volatility\", color='black', alpha=0.7)\n",
        "    plt.plot(preds[:200], label=f\"GARCH({best_p},{best_q})\", color='blue', linestyle='--')\n",
        "    plt.title(f\"Optimized GARCH({best_p},{best_q}) Benchmark\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "68nvrtOR0EO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- RUN IT ---\n",
        "run_optimized_garch(TARGET_SYMBOLS)"
      ],
      "metadata": {
        "id": "nQLnS0JxAg4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pBWzqFeAhkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}